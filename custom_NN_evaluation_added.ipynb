{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "import numpy as np\n",
    "from NN_n_layers_custom import formulas as f\n",
    "from NN_n_layers_custom import preprocessing as enc\n",
    "\n",
    "\n",
    "def logging_time(original_fn): #decorator_review\n",
    "    def wrapper_fn(*args, **kargs):\n",
    "        start_time = time.time()\n",
    "        result = original_fn(*args, **kargs)\n",
    "        end_time = time.time()\n",
    "\n",
    "        print(f\"Working Time[{original_fn.__name__}] : {end_time - start_time}\")\n",
    "        return result\n",
    "    return wrapper_fn\n",
    "\n",
    "\n",
    "class Custom_NN_model:\n",
    "    def __init__(self, X, Y, nLayer = 7, nNode = 500):\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            raise TypeError(\"X's type should be <%s>. But it's %s\"%(\"np ndarray\",type(X)))\n",
    "        elif not isinstance(Y, np.ndarray):\n",
    "            raise TypeError(\"Y's type should be <%s>. But it's %s\"%(\"np ndarray\",type(Y)))\n",
    "        assert len(X) == len(Y), \"The number of examples should be equal to the number of ground truth\"\n",
    "        \n",
    "        self.nLayer = nLayer\n",
    "        self.nNode  = nNode\n",
    "        self.XYsetter(X, Y)\n",
    "        self.param = {}\n",
    "        self.loss = []\n",
    "        self.lr = 0.000001 # learning rate\n",
    "        self.out_A = {}\n",
    "        self.in_Z = {}\n",
    "        self.Delta = {}\n",
    "        self.gradient = {}\n",
    "        \n",
    "        ## 데이터의 차원이 바뀌면 달라져야 한다... classification feature 에만 적용 가능\n",
    "        #self.dims = [self.X.shape[1], self.nNode, self.Y.shape[1]]\n",
    "\n",
    "\n",
    "        # 레이어마다 들어갈 함수 순서대로 지정\n",
    "        self.function = [f.Relu, f.Sigmoid, f.Sigmoid, f.Sigmoid, f.Sigmoid, f.Sigmoid, f.Sigmoid]\n",
    "        self.dfunction = [f.dRelu, f.dSigmoid, f.Sigmoid, f.Sigmoid, f.Sigmoid, f.Sigmoid, f.Sigmoid, f.dCrossEntropy]\n",
    "\n",
    "    def XYsetter(self, x,y):\n",
    "        def unison_random_shuffle(*arg):\n",
    "            for i in range(len(arg)):\n",
    "                assert len(arg[0]) == len(arg[i])\n",
    "            p = np.random.permutation(len(arg[0]))\n",
    "            return ( entry[p] for entry in arg)   \n",
    "        \n",
    "        self.X, self.Y = unison_random_shuffle(x, y)\n",
    "\n",
    "        self.sam = self.Y.shape[0] # # The number of training samples we have(or the batch size)\n",
    "        self.loss = []\n",
    "\n",
    "    def param_init(self):\n",
    "        for i in range(1, self.nLayer +1):\n",
    "            \n",
    "            if i == 1: # First Layer\n",
    "                self.param[f'W{i}'] = np.random.normal(size = self.dims[0] * self.dims[1]).reshape(self.dims[0], self.dims[1])\n",
    "                self.param[f'b{i}'] = np.ones(self.dims[1]) #dimension 이 다르지만 문제가 발생하지는 않는다.\n",
    "            elif i == self.nLayer: # Last Layer\n",
    "                self.param[f'W{i}'] = np.random.normal(size = self.dims[1] * self.dims[2]).reshape(self.dims[1], self.dims[2])\n",
    "                self.param[f'b{i}'] = np.ones(self.dims[2])\n",
    "            else: # Layers in between\n",
    "                self.param[f'W{i}'] = np.random.normal(size = self.dims[1] * self.dims[1]).reshape(self.dims[1], self.dims[1])\n",
    "                self.param[f'b{i}'] = np.ones(self.dims[1])\n",
    "        \n",
    "        return self.param # for testing __field is not a private field\n",
    "\n",
    "    \n",
    "    def frontprop(self, X, Y): # get A, Z\n",
    "        self.out_A['A0'] = X\n",
    "        \n",
    "        for i in range(1, self.nLayer+1):\n",
    "            self.in_Z[f'Z{i}'] = np.matmul(self.out_A[f'A{i-1}'], self.param[f'W{i}']) + self.param[f'b{i}']\n",
    "            self.out_A[f'A{i}'] = self.function[i-1](self.in_Z[f'Z{i}'])\n",
    "        \n",
    "        y_hypothesis = self.out_A[f'A{self.nLayer}']\n",
    "        print(y_hypothesis)\n",
    "        loss_one_iter = np.sum(f.CrossEntropy(y_hypothesis, Y))\n",
    "        self.loss.append(loss_one_iter)\n",
    "\n",
    "    def backprop(self, Y):\n",
    "        #delta of last layer\n",
    "        dLoss = f.dCrossEntropy # last entry of the dfunction list is the derivative of loss function\n",
    "\n",
    "        self.Delta[f'D{self.nLayer}'] = dLoss(self.out_A[f'A{self.nLayer}'], Y) * self.dfunction[self.nLayer-1](self.in_Z[f'Z{self.nLayer}'])\n",
    "        \n",
    "        for i in reversed(range(1, self.nLayer)):\n",
    "            #rest\n",
    "            self.Delta[f'D{i}'] = np.dot(self.Delta[f'D{i+1}'], self.param[f'W{i+1}'].T) * self.dfunction[i-1](self.in_Z[f'Z{i}'])\n",
    "\n",
    "    def gradients(self): # get the gradients\n",
    "        for i in range(1, self.nLayer + 1):\n",
    "            self.gradient[f'dW{i}'] = (1/self.sam) * np.matmul(self.out_A[f'A{i-1}'].T, self.Delta[f'D{i}'])\n",
    "            self.gradient[f'db{i}'] = (1/self.sam) * np.sum(self.Delta[f'D{i}'], axis = 0, keepdims=True)\n",
    "\n",
    "    def gradientdescent(self): # execute gradient descent\n",
    "        for i in range(1, self.nLayer + 1):\n",
    "            print('shape:',self.gradient['dW1'].shape)\n",
    "            self.param[f'W{i}'] -= self.lr * self.gradient[f'dW{i}']\n",
    "            self.param[f'b{i}'] -= self.lr * np.squeeze(self.gradient[f'db{i}'])\n",
    "\n",
    "    def minibatch_setter(self, batch_size):\n",
    "        self.XYsetter(self.X, self.Y) #shuffling 해줌\n",
    "        nBatch = int(self.X.shape[0]/batch_size)\n",
    "\n",
    "        ## use index to mini_batches, for saving memory\n",
    "        idx = [(i * batch_size, (i+1) *batch_size) if i != nBatch - 1 else (i*batch_size, self.X.shape[0]) for i in range(nBatch)]\n",
    "        return idx\n",
    "\n",
    "    @logging_time\n",
    "    def minibatch_gradientdescent(self, batch_size = 32, nEpoch = 3000):\n",
    "        for i in range(nEpoch):\n",
    "            idx = self.minibatch_setter(batch_size)\n",
    "            cnt = 0\n",
    "            for batch_begin, batch_end in idx:\n",
    "                cnt += 1\n",
    "                self.sam = self.X[batch_begin:batch_end].shape[0]\n",
    "\n",
    "                ##change to onehot encoding\n",
    "                X = enc.labeltoOneHotVector(self.X[batch_begin:batch_end], int(max(self.X))+1)\n",
    "                Y = enc.labeltoOneHotVector(self.Y[batch_begin:batch_end], int(max(self.Y))+1)\n",
    "                self.dims = [X.shape[1], self.nNode, Y.shape[1]]\n",
    "\n",
    "                self.param_init()\n",
    "                self.frontprop(X, Y)\n",
    "                print(\"Loss : \",self.loss[-1])\n",
    "                self.backprop(Y)\n",
    "                self.gradients()\n",
    "                self.gradientdescent()\n",
    "                print(\"%.2f percent done in one epoch\"% (cnt/len(idx) * 100))\n",
    "            \n",
    "            print(\"\\n===========================================================================\")\n",
    "            print(\"=======epoch done=========\")\n",
    "            print(\"Total progress : %.3f\\n\" % ((len(idx) * i + cnt) / (nEpoch * len(idx)) *100) )\n",
    "        \n",
    "        return self.param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'NN_3_layers_custom'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-7bbacf12d347>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mNN_n_layers_custom\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNN_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mNN_n_layers_custom\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpreprocessing\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0menc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'David'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Seop'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Nash'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Seop'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Will'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Workplace/JediTemple/custom_NN_net/NN_Custom/NN_n_layers_custom/NN_model/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mNN_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Workplace/JediTemple/custom_NN_net/NN_Custom/NN_n_layers_custom/NN_model/NN_model.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mNN_3_layers_custom\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mformulas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mNN_3_layers_custom\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpreprocessing\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0menc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'NN_3_layers_custom'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from NN_n_layers_custom import NN_model\n",
    "from NN_n_layers_custom import preprocessing as enc\n",
    "X = np.array(['David', 'Seop', 'Nash', 'Seop', 'Will'])\n",
    "Y = np.array(['1', '0', '1', '0', '1'])\n",
    "\n",
    "X= enc.labelencoder(X)\n",
    "Y= enc.labelencoder(Y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
